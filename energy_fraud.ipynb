{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with the first column as datetime and handle comma as decimal separator\n",
    "file_path = 'LD2011_2014.txt'\n",
    "data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=';',                        # Semicolon-separated\n",
    "    header=0,                       # Use the first row as the header\n",
    "    parse_dates=[0],                # Parse the first column as datetime\n",
    "    index_col=0,                    # Set the first column as the index\n",
    "    dayfirst=True,                  # Assume the date format is day-first\n",
    "    decimal=','                     # Use comma as decimal separator\n",
    ")\n",
    "\n",
    "# Display the first few rows to confirm correct loading\n",
    "#print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert kW to kWh by dividing each value by 4\n",
    "data_kwh = data / 4\n",
    "\n",
    "# Display the first few rows to verify\n",
    "#print(data_kwh.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data by date to cover different seasons\n",
    "train_data = data_kwh['2011-01-01':'2012-12-31']\n",
    "val_data = data_kwh['2013-01-01':'2013-12-31']\n",
    "test_data = data_kwh['2014-01-01':'2014-12-31']\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to inject anomalies and label them\n",
    "def inject_and_label_anomalies(data, anomaly_percentage=0.01, spike_factor=3, prolonged_anomaly_duration=4):\n",
    "    \"\"\"\n",
    "    Inject synthetic anomalies by randomly selecting points and adding spikes, dips, or prolonged high consumption.\n",
    "    \n",
    "    Parameters:\n",
    "    - data (pd.DataFrame): The dataset where anomalies are injected.\n",
    "    - anomaly_percentage (float): The percentage of points to modify with anomalies.\n",
    "    - spike_factor (float): The multiplier for spikes to simulate anomalies.\n",
    "    - prolonged_anomaly_duration (int): Number of consecutive data points to create prolonged high consumption anomalies.\n",
    "    \n",
    "    Returns:\n",
    "    - data_with_anomalies (pd.DataFrame): Dataset with synthetic anomalies and anomaly labels.\n",
    "    \"\"\"\n",
    "    # Ensure the index is in datetime format\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original data\n",
    "    data_with_anomalies = data.copy()\n",
    "    \n",
    "    # Add a column for anomaly labels (0 = normal, 1 = anomaly)\n",
    "    data_with_anomalies['anomaly'] = 0\n",
    "    \n",
    "    # Calculate the total number of anomalies to inject\n",
    "    num_anomalies = int(len(data) * anomaly_percentage)\n",
    "    \n",
    "    # Inject short-term spikes and dips\n",
    "    short_term_indices = np.random.choice(data.index, size=num_anomalies, replace=False)\n",
    "    for idx in short_term_indices:\n",
    "        col = np.random.choice(data.columns[:-1])  # Avoid 'anomaly' column if added\n",
    "        if np.random.rand() > 0.5:\n",
    "            # Spike: Increase value\n",
    "            data_with_anomalies.loc[idx, col] *= spike_factor\n",
    "        else:\n",
    "            # Dip: Decrease value\n",
    "            data_with_anomalies.loc[idx, col] /= spike_factor\n",
    "        \n",
    "        # Mark as anomaly\n",
    "        data_with_anomalies.loc[idx, 'anomaly'] = 1\n",
    "    \n",
    "    # Inject prolonged high consumption anomalies\n",
    "    prolonged_indices = np.random.choice(data.index[:-prolonged_anomaly_duration], size=num_anomalies // 2, replace=False)\n",
    "    for idx in prolonged_indices:\n",
    "        # Apply prolonged high consumption by setting a range of consecutive rows\n",
    "        for i in range(prolonged_anomaly_duration):\n",
    "            col = np.random.choice(data.columns[:-1])  # Avoid 'anomaly' column if added\n",
    "            # Ensure idx + timedelta works by using the datetime index\n",
    "            data_with_anomalies.loc[idx + pd.Timedelta(minutes=15 * i), col] *= spike_factor\n",
    "            data_with_anomalies.loc[idx + pd.Timedelta(minutes=15 * i), 'anomaly'] = 1\n",
    "    \n",
    "    return data_with_anomalies\n",
    "\n",
    "# Inject anomalies into validation and test sets and label them\n",
    "val_data_with_anomalies = inject_and_label_anomalies(val_data, anomaly_percentage=0.01, spike_factor=3)\n",
    "test_data_with_anomalies = inject_and_label_anomalies(test_data, anomaly_percentage=0.01, spike_factor=3)\n",
    "\n",
    "# Display sample rows with anomalies for verification\n",
    "print(\"Sample validation data with anomalies:\")\n",
    "#print(val_data_with_anomalies[val_data_with_anomalies['anomaly'] == 1].sample(5))\n",
    "\n",
    "print(\"Sample test data with anomalies:\")\n",
    "#print(test_data_with_anomalies[test_data_with_anomalies['anomaly'] == 1].sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_with_anomalies['anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming `data_kwh` is the original DataFrame with values converted to kWh\n",
    "\n",
    "# Step 1: Split the data by date and retain the 'anomaly' column if it exists\n",
    "train_data = data_kwh['2011-01-01':'2012-12-31'].copy()\n",
    "val_data_with_anomalies = data_kwh['2013-01-01':'2013-12-31'].copy()\n",
    "test_data_with_anomalies = data_kwh['2014-01-01':'2014-12-31'].copy()\n",
    "\n",
    "# Separate 'anomaly' columns for future evaluation if they exist\n",
    "train_anomaly = train_data['anomaly'].copy() if 'anomaly' in train_data.columns else None\n",
    "val_anomaly = val_data_with_anomalies['anomaly'].copy() if 'anomaly' in val_data_with_anomalies.columns else None\n",
    "\n",
    "# Drop 'anomaly' column temporarily for model input\n",
    "X_train = train_data.drop(columns=['anomaly'], errors='ignore')\n",
    "X_val = val_data_with_anomalies.drop(columns=['anomaly'], errors='ignore')\n",
    "\n",
    "# Step 2: Train the Isolation Forest on X_train\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Step 3: Predict anomalies on the validation set\n",
    "# Store predictions in a new DataFrame to avoid overwriting val_data_with_anomalies\n",
    "predictions = pd.DataFrame(index=val_data_with_anomalies.index)\n",
    "predictions['predicted_anomaly'] = iso_forest.predict(X_val)\n",
    "predictions['predicted_anomaly'] = predictions['predicted_anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "# Re-attach original 'anomaly' column and predictions for evaluation\n",
    "val_data_with_anomalies = val_data_with_anomalies.assign(anomaly=val_anomaly)\n",
    "val_data_with_anomalies['predicted_anomaly'] = predictions['predicted_anomaly']\n",
    "\n",
    "# Fill NaN values in 'anomaly' and 'predicted_anomaly' columns if they exist\n",
    "val_data_with_anomalies['anomaly'] = val_data_with_anomalies['anomaly'].fillna(0)\n",
    "val_data_with_anomalies['predicted_anomaly'] = val_data_with_anomalies['predicted_anomaly'].fillna(0)\n",
    "\n",
    "# Display a sample of results to confirm NaNs are handled\n",
    "print(\"Sample of validation data with anomalies and predicted anomalies after filling NaNs:\")\n",
    "print(val_data_with_anomalies[['anomaly', 'predicted_anomaly']].sample(10))\n",
    "\n",
    "# Step 4: Evaluate Model Performance\n",
    "if 'anomaly' in val_data_with_anomalies.columns:\n",
    "    print(\"Classification Report for Validation Data:\")\n",
    "    print(classification_report(val_data_with_anomalies['anomaly'], val_data_with_anomalies['predicted_anomaly']))\n",
    "else:\n",
    "    print(\"'anomaly' column missing in validation data. Cannot generate classification report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_with_anomalies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_with_anomalies['anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (70175, 370)\n",
      "Validation data shape: (35040, 370)\n",
      "Test data shape: (35040, 370)\n",
      "Sample validation data with anomalies:\n",
      "                       MT_001    MT_002    MT_003     MT_004     MT_005  \\\n",
      "2013-09-03 13:45:00  3.489848  8.357041  0.434405  24.390244  10.670732   \n",
      "2013-03-15 01:45:00  0.634518  5.689900  0.651607  22.865854  11.280488   \n",
      "2013-12-07 04:00:00  0.634518  5.512091  0.434405  24.898374  14.634146   \n",
      "2013-08-27 12:00:00  0.317259  8.001422  0.434405  30.995935  14.024390   \n",
      "2013-12-29 23:30:00  0.634518  6.223329  0.434405  43.699187  21.036585   \n",
      "\n",
      "                        MT_006    MT_007     MT_008     MT_009     MT_010  \\\n",
      "2013-09-03 13:45:00  49.107143  3.533070  83.333333  23.164336  15.322581   \n",
      "2013-03-15 01:45:00  33.482143  1.130582  44.612795   9.615385  15.591398   \n",
      "2013-12-07 04:00:00  36.458333  0.706614  51.346801  11.363636   9.139785   \n",
      "2013-08-27 12:00:00  47.619048  5.652911  77.441077   8.304196   9.946237   \n",
      "2013-12-29 23:30:00  68.452381  1.413228  69.865320  13.548951  19.892473   \n",
      "\n",
      "                     ...   MT_362       MT_363       MT_364     MT_365  \\\n",
      "2013-09-03 13:45:00  ...  31400.0  1328.059072  1914.772727  39.765319   \n",
      "2013-03-15 01:45:00  ...   3425.0   140.295359   227.272727   3.911343   \n",
      "2013-12-07 04:00:00  ...   3300.0   148.734177   227.272727   4.889179   \n",
      "2013-08-27 12:00:00  ...  26025.0  1318.565401  1727.272727  41.069100   \n",
      "2013-12-29 23:30:00  ...   9750.0   496.835443   642.045455  24.119948   \n",
      "\n",
      "                       MT_366      MT_367     MT_368      MT_369       MT_370  \\\n",
      "2013-09-03 13:45:00  3.364541  198.639157  31.719533  286.656891  5589.189189   \n",
      "2013-03-15 01:45:00  2.340550  173.836699  47.579299  176.686217  3567.567568   \n",
      "2013-12-07 04:00:00  2.047981   57.945566  20.450751  162.756598  5405.405405   \n",
      "2013-08-27 12:00:00  2.925688  175.153644  35.058431  244.501466  4697.297297   \n",
      "2013-12-29 23:30:00  3.949678   66.505707  11.268781  175.769795  3932.432432   \n",
      "\n",
      "                     anomaly  \n",
      "2013-09-03 13:45:00        1  \n",
      "2013-03-15 01:45:00        1  \n",
      "2013-12-07 04:00:00        1  \n",
      "2013-08-27 12:00:00        1  \n",
      "2013-12-29 23:30:00        1  \n",
      "\n",
      "[5 rows x 371 columns]\n",
      "Sample of validation data with anomalies and predicted anomalies after filling NaNs:\n",
      "                     anomaly  predicted_anomaly\n",
      "2013-08-30 14:15:00        0                  0\n",
      "2013-08-26 20:30:00        0                  0\n",
      "2013-01-17 21:00:00        0                  0\n",
      "2013-05-30 20:45:00        0                  0\n",
      "2013-05-28 08:15:00        0                  0\n",
      "2013-04-24 13:45:00        0                  0\n",
      "2013-01-19 21:30:00        0                  0\n",
      "2013-12-30 15:15:00        0                  0\n",
      "2013-02-12 20:15:00        1                  0\n",
      "2013-11-22 23:30:00        0                  0\n",
      "Classification Report for Validation Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.96     33996\n",
      "           1       0.02      0.04      0.03      1044\n",
      "\n",
      "    accuracy                           0.93     35040\n",
      "   macro avg       0.50      0.50      0.50     35040\n",
      "weighted avg       0.94      0.93      0.94     35040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the data with the first column as datetime and handle comma as decimal separator\n",
    "file_path = 'LD2011_2014.txt'\n",
    "data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=';',                        # Semicolon-separated\n",
    "    header=0,                       # Use the first row as the header\n",
    "    parse_dates=[0],                # Parse the first column as datetime\n",
    "    index_col=0,                    # Set the first column as the index\n",
    "    dayfirst=True,                  # Assume the date format is day-first\n",
    "    decimal=','                     # Use comma as decimal separator\n",
    ")\n",
    "\n",
    "# Convert kW to kWh by dividing each value by 4\n",
    "data_kwh = data / 4\n",
    "\n",
    "# Force datetime conversion with 'errors=coerce' to identify problematic dates\n",
    "data_kwh.index = pd.to_datetime(data_kwh.index, errors='coerce')\n",
    "\n",
    "# Identify and display rows where the datetime index is invalid (NaT)\n",
    "invalid_rows = data_kwh[data_kwh.index.isna()]\n",
    "if not invalid_rows.empty:\n",
    "    print(\"Rows with invalid datetime format:\")\n",
    "    print(invalid_rows)\n",
    "    # Drop rows with NaT index\n",
    "    data_kwh = data_kwh.dropna(subset=[data_kwh.index.name])\n",
    "\n",
    "# Split the data by date to cover different seasons\n",
    "train_data = data_kwh['2011-01-01':'2012-12-31']\n",
    "val_data = data_kwh['2013-01-01':'2013-12-31']\n",
    "test_data = data_kwh['2014-01-01':'2014-12-31']\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "print(\"Validation data shape:\", val_data.shape)\n",
    "print(\"Test data shape:\", test_data.shape)\n",
    "\n",
    "# Define a function to inject anomalies and label them\n",
    "def inject_and_label_anomalies(data, anomaly_percentage=0.01, spike_factor=3, prolonged_anomaly_duration=4):\n",
    "    # Ensure the index is in datetime format\n",
    "    data.index = pd.to_datetime(data.index, errors='coerce').tz_localize(None)\n",
    "    \n",
    "    # Create a copy to avoid modifying the original data\n",
    "    data_with_anomalies = data.copy()\n",
    "    \n",
    "    # Add a column for anomaly labels (0 = normal, 1 = anomaly)\n",
    "    data_with_anomalies['anomaly'] = 0\n",
    "    \n",
    "    # Calculate the total number of anomalies to inject\n",
    "    num_anomalies = int(len(data) * anomaly_percentage)\n",
    "    \n",
    "    # Inject short-term spikes and dips\n",
    "    short_term_indices = np.random.choice(data.index.dropna(), size=num_anomalies, replace=False)\n",
    "    for idx in short_term_indices:\n",
    "        col = np.random.choice(data.columns)  # Choose a random column\n",
    "        if np.random.rand() > 0.5:\n",
    "            # Spike: Increase value\n",
    "            data_with_anomalies.loc[idx, col] *= spike_factor\n",
    "        else:\n",
    "            # Dip: Decrease value\n",
    "            data_with_anomalies.loc[idx, col] /= spike_factor\n",
    "        # Mark as anomaly\n",
    "        data_with_anomalies.loc[idx, 'anomaly'] = 1\n",
    "    \n",
    "    # Inject prolonged high consumption anomalies\n",
    "    prolonged_indices = np.random.choice(data.index.dropna()[:-prolonged_anomaly_duration], size=num_anomalies // 2, replace=False)\n",
    "    for idx in prolonged_indices:\n",
    "        for i in range(prolonged_anomaly_duration):\n",
    "            col = np.random.choice(data.columns)  # Choose a random column\n",
    "            timestamp = idx + pd.Timedelta(minutes=15 * i)\n",
    "            if timestamp in data_with_anomalies.index:\n",
    "                data_with_anomalies.loc[timestamp, col] *= spike_factor\n",
    "                data_with_anomalies.loc[timestamp, 'anomaly'] = 1\n",
    "    \n",
    "    return data_with_anomalies\n",
    "\n",
    "# Inject anomalies into validation and test sets and label them\n",
    "val_data_with_anomalies = inject_and_label_anomalies(val_data, anomaly_percentage=0.01, spike_factor=3)\n",
    "test_data_with_anomalies = inject_and_label_anomalies(test_data, anomaly_percentage=0.01, spike_factor=3)\n",
    "\n",
    "# Display sample rows with anomalies for verification\n",
    "print(\"Sample validation data with anomalies:\")\n",
    "print(val_data_with_anomalies[val_data_with_anomalies['anomaly'] == 1].sample(5))\n",
    "\n",
    "# Separate 'anomaly' columns for future evaluation if they exist\n",
    "train_anomaly = train_data['anomaly'].copy() if 'anomaly' in train_data.columns else None\n",
    "val_anomaly = val_data_with_anomalies['anomaly'].copy() if 'anomaly' in val_data_with_anomalies.columns else None\n",
    "\n",
    "# Drop 'anomaly' column temporarily for model input\n",
    "X_train = train_data.drop(columns=['anomaly'], errors='ignore')\n",
    "X_val = val_data_with_anomalies.drop(columns=['anomaly'], errors='ignore')\n",
    "\n",
    "# Train the Isolation Forest on X_train\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "\n",
    "# Predict anomalies on the validation set\n",
    "# Store predictions in a new DataFrame to avoid overwriting val_data_with_anomalies\n",
    "predictions = pd.DataFrame(index=val_data_with_anomalies.index)\n",
    "predictions['predicted_anomaly'] = iso_forest.predict(X_val)\n",
    "predictions['predicted_anomaly'] = predictions['predicted_anomaly'].apply(lambda x: 1 if x == -1 else 0)\n",
    "\n",
    "# Re-attach original 'anomaly' column and predictions for evaluation\n",
    "val_data_with_anomalies['anomaly'] = val_anomaly\n",
    "val_data_with_anomalies['predicted_anomaly'] = predictions['predicted_anomaly']\n",
    "\n",
    "# Fill NaN values in 'anomaly' and 'predicted_anomaly' columns if they exist\n",
    "val_data_with_anomalies['anomaly'] = val_data_with_anomalies['anomaly'].fillna(0)\n",
    "val_data_with_anomalies['predicted_anomaly'] = val_data_with_anomalies['predicted_anomaly'].fillna(0)\n",
    "\n",
    "# Display a sample of results to confirm NaNs are handled\n",
    "print(\"Sample of validation data with anomalies and predicted anomalies after filling NaNs:\")\n",
    "print(val_data_with_anomalies[['anomaly', 'predicted_anomaly']].sample(10))\n",
    "\n",
    "# Evaluate Model Performance\n",
    "if 'anomaly' in val_data_with_anomalies.columns:\n",
    "    print(\"Classification Report for Validation Data:\")\n",
    "    print(classification_report(val_data_with_anomalies['anomaly'], val_data_with_anomalies['predicted_anomaly']))\n",
    "else:\n",
    "    print(\"'anomaly' column missing in validation data. Cannot generate classification report.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0413 - val_loss: 54631.5000\n",
      "Epoch 2/100\n",
      "1095/1095 [==============================] - 5s 5ms/step - loss: 0.0087 - val_loss: 54631.4805\n",
      "Epoch 3/100\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0064 - val_loss: 54631.4805\n",
      "Epoch 4/100\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0061 - val_loss: 54631.4844\n",
      "Epoch 5/100\n",
      "1095/1095 [==============================] - 6s 5ms/step - loss: 0.0059 - val_loss: 54631.4844\n",
      "Epoch 6/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0057 - val_loss: 54631.4844\n",
      "Epoch 7/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0056 - val_loss: 54631.4883\n",
      "Epoch 8/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0054 - val_loss: 54631.4844\n",
      "Epoch 9/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0053 - val_loss: 54631.4844\n",
      "Epoch 10/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0052 - val_loss: 54631.4883\n",
      "Epoch 11/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0051 - val_loss: 54631.4883\n",
      "Epoch 12/100\n",
      "1095/1095 [==============================] - 12s 11ms/step - loss: 0.0051 - val_loss: 54631.4883\n",
      "Epoch 13/100\n",
      "1095/1095 [==============================] - 14s 13ms/step - loss: 0.0050 - val_loss: 54631.4883\n",
      "Epoch 14/100\n",
      "1095/1095 [==============================] - 9s 8ms/step - loss: 0.0048 - val_loss: 54631.4883\n",
      "Epoch 15/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0047 - val_loss: 54631.4883\n",
      "Epoch 16/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0046 - val_loss: 54631.4883\n",
      "Epoch 17/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0045 - val_loss: 54631.4883\n",
      "Epoch 18/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0044 - val_loss: 54631.4883\n",
      "Epoch 19/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0043 - val_loss: 54631.4883\n",
      "Epoch 20/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0043 - val_loss: 54631.4883\n",
      "Epoch 21/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0042 - val_loss: 54631.4883\n",
      "Epoch 22/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0042 - val_loss: 54631.4883\n",
      "Epoch 23/100\n",
      "1095/1095 [==============================] - 6s 6ms/step - loss: 0.0042 - val_loss: 54631.4883\n",
      "Epoch 24/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0041 - val_loss: 54631.4883\n",
      "Epoch 25/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0041 - val_loss: 54631.4883\n",
      "Epoch 26/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0040 - val_loss: 54631.4883\n",
      "Epoch 27/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0040 - val_loss: 54631.4883\n",
      "Epoch 28/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0039 - val_loss: 54631.4883\n",
      "Epoch 29/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0039 - val_loss: 54631.4883\n",
      "Epoch 30/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0039 - val_loss: 54631.4883\n",
      "Epoch 31/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0038 - val_loss: 54631.4883\n",
      "Epoch 32/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0038 - val_loss: 54631.4883\n",
      "Epoch 33/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0037 - val_loss: 54631.4883\n",
      "Epoch 34/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0037 - val_loss: 54631.4883\n",
      "Epoch 35/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0036 - val_loss: 54631.4883\n",
      "Epoch 36/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0036 - val_loss: 54631.4883\n",
      "Epoch 37/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0035 - val_loss: 54631.4883\n",
      "Epoch 38/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0035 - val_loss: 54631.4883\n",
      "Epoch 39/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0034 - val_loss: 54631.4883\n",
      "Epoch 40/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0034 - val_loss: 54631.4883\n",
      "Epoch 41/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0033 - val_loss: 54631.4883\n",
      "Epoch 42/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0033 - val_loss: 54631.4883\n",
      "Epoch 43/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0032 - val_loss: 54631.4883\n",
      "Epoch 44/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0032 - val_loss: 54631.4883\n",
      "Epoch 45/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0032 - val_loss: 54631.4883\n",
      "Epoch 46/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0031 - val_loss: 54631.4883\n",
      "Epoch 47/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0031 - val_loss: 54631.4883\n",
      "Epoch 48/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0031 - val_loss: 54631.4883\n",
      "Epoch 49/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0031 - val_loss: 54631.4883\n",
      "Epoch 50/100\n",
      "1095/1095 [==============================] - 10s 9ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 51/100\n",
      "1095/1095 [==============================] - 15s 13ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 52/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 53/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 54/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 55/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0030 - val_loss: 54631.4883\n",
      "Epoch 56/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 57/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 58/100\n",
      "1095/1095 [==============================] - 8s 8ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 59/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 60/100\n",
      "1095/1095 [==============================] - 15s 13ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 61/100\n",
      "1095/1095 [==============================] - 11s 10ms/step - loss: 0.0029 - val_loss: 54631.4883\n",
      "Epoch 62/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 63/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 64/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 65/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 66/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 67/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 68/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 69/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 70/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 71/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0028 - val_loss: 54631.4883\n",
      "Epoch 72/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 73/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 74/100\n",
      "1095/1095 [==============================] - 11s 10ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 75/100\n",
      "1095/1095 [==============================] - 15s 14ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 76/100\n",
      "1095/1095 [==============================] - 15s 14ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 77/100\n",
      "1095/1095 [==============================] - 15s 14ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 78/100\n",
      "1095/1095 [==============================] - 8s 7ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 79/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 80/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 81/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 82/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 83/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 84/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 85/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 86/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 87/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 88/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 89/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0027 - val_loss: 54631.4883\n",
      "Epoch 90/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 91/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 92/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 93/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 94/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 95/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4922\n",
      "Epoch 96/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 97/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 98/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 99/100\n",
      "1095/1095 [==============================] - 7s 6ms/step - loss: 0.0026 - val_loss: 54631.4883\n",
      "Epoch 100/100\n",
      "1095/1095 [==============================] - 7s 7ms/step - loss: 0.0026 - val_loss: 54631.4961\n",
      "1092/1092 [==============================] - 4s 3ms/step\n",
      "1092/1092 [==============================] - 3s 3ms/step\n",
      "1092/1092 [==============================] - 3s 3ms/step\n",
      "Validation Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.90      0.92     33197\n",
      "         1.0       0.05      0.09      0.06      1747\n",
      "\n",
      "    accuracy                           0.86     34944\n",
      "   macro avg       0.50      0.50      0.49     34944\n",
      "weighted avg       0.90      0.86      0.88     34944\n",
      "\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.90      0.92     33197\n",
      "         1.0       0.05      0.10      0.07      1747\n",
      "\n",
      "    accuracy                           0.86     34944\n",
      "   macro avg       0.50      0.50      0.50     34944\n",
      "weighted avg       0.91      0.86      0.88     34944\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load and preprocess the data\n",
    "file_path = 'LD2011_2014.txt'\n",
    "data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=';',                        \n",
    "    header=0,                       \n",
    "    parse_dates=[0],                \n",
    "    index_col=0,                    \n",
    "    dayfirst=True,                  \n",
    "    decimal=','                     \n",
    ")\n",
    "data_kwh = data / 4\n",
    "\n",
    "# Split the data\n",
    "train_data = data_kwh['2011-01-01':'2012-12-31']\n",
    "val_data = data_kwh['2013-01-01':'2013-12-31']\n",
    "test_data = data_kwh['2014-01-01':'2014-12-31']\n",
    "\n",
    "# Normalize with MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Inject Anomalies for Validation and Test Sets\n",
    "def inject_anomalies(data, anomaly_percentage=0.05, spike_factor=3, prolonged_duration=6):\n",
    "    data_with_anomalies = data.copy()\n",
    "    n_anomalies = int(anomaly_percentage * len(data))\n",
    "    anomaly_indices = np.random.choice(len(data), n_anomalies, replace=False)\n",
    "\n",
    "    for idx in anomaly_indices:\n",
    "        col = np.random.randint(0, data.shape[1])  \n",
    "        if np.random.rand() < 0.5:\n",
    "            data_with_anomalies[idx, col] *= spike_factor  \n",
    "        else:\n",
    "            data_with_anomalies[idx, col] /= spike_factor  \n",
    "\n",
    "        if np.random.rand() < 0.3:\n",
    "            for i in range(prolonged_duration):\n",
    "                if idx + i < len(data_with_anomalies):\n",
    "                    data_with_anomalies[idx + i, col] *= spike_factor\n",
    "\n",
    "    return data_with_anomalies, anomaly_indices\n",
    "\n",
    "val_data_with_anomalies, val_anomaly_indices = inject_anomalies(val_data, anomaly_percentage=0.05)\n",
    "test_data_with_anomalies, test_anomaly_indices = inject_anomalies(test_data, anomaly_percentage=0.05)\n",
    "\n",
    "# Define a deeper Autoencoder model\n",
    "input_dim = train_data.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoder = Dense(encoding_dim, activation=\"relu\")(input_layer)\n",
    "encoder = Dense(16, activation=\"relu\")(encoder)\n",
    "encoder = Dense(8, activation=\"relu\")(encoder)\n",
    "decoder = Dense(16, activation=\"relu\")(encoder)\n",
    "decoder = Dense(encoding_dim, activation=\"relu\")(decoder)\n",
    "decoder = Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\")\n",
    "\n",
    "# Train with reduced batch size\n",
    "history = autoencoder.fit(train_data, train_data,\n",
    "                          epochs=100,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(val_data, val_data),\n",
    "                          verbose=1)\n",
    "\n",
    "# Set a threshold using validation data MSE\n",
    "reconstructions = autoencoder.predict(val_data)\n",
    "mse = np.mean(np.power(val_data - reconstructions, 2), axis=1)\n",
    "threshold = np.percentile(mse, 90)\n",
    "\n",
    "# Detect Anomalies\n",
    "# Validation Set\n",
    "reconstructions = autoencoder.predict(val_data_with_anomalies)\n",
    "mse = np.mean(np.power(val_data_with_anomalies - reconstructions, 2), axis=1)\n",
    "predicted_anomalies_val = mse > threshold\n",
    "\n",
    "# Test Set\n",
    "reconstructions = autoencoder.predict(test_data_with_anomalies)\n",
    "mse = np.mean(np.power(test_data_with_anomalies - reconstructions, 2), axis=1)\n",
    "predicted_anomalies_test = mse > threshold\n",
    "\n",
    "# Evaluate results\n",
    "val_true_labels = np.zeros(len(val_data_with_anomalies))\n",
    "val_true_labels[val_anomaly_indices] = 1\n",
    "print(\"Validation Set Classification Report:\")\n",
    "print(classification_report(val_true_labels, predicted_anomalies_val))\n",
    "\n",
    "test_true_labels = np.zeros(len(test_data_with_anomalies))\n",
    "test_true_labels[test_anomaly_indices] = 1\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(test_true_labels, predicted_anomalies_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "50 epochs\n",
    "Validation Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.90      0.92     33197\n",
    "         1.0       0.05      0.11      0.07      1747\n",
    "\n",
    "    accuracy                           0.86     34944\n",
    "   macro avg       0.50      0.50      0.50     34944\n",
    "weighted avg       0.91      0.86      0.88     34944\n",
    "\n",
    "Test Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.90      0.92     33197\n",
    "         1.0       0.05      0.10      0.07      1747\n",
    "\n",
    "    accuracy                           0.86     34944\n",
    "   macro avg       0.50      0.50      0.50     34944\n",
    "weighted avg       0.91      0.86      0.88     34944\n",
    "\n",
    "100 epochs\n",
    "Validation Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.90      0.92     33197\n",
    "         1.0       0.05      0.09      0.06      1747\n",
    "\n",
    "    accuracy                           0.86     34944\n",
    "   macro avg       0.50      0.50      0.49     34944\n",
    "weighted avg       0.90      0.86      0.88     34944\n",
    "\n",
    "Test Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "         0.0       0.95      0.90      0.92     33197\n",
    "         1.0       0.05      0.10      0.07      1747\n",
    "\n",
    "    accuracy                           0.86     34944\n",
    "   macro avg       0.50      0.50      0.50     34944\n",
    "weighted avg       0.91      0.86      0.88     34944\n",
    "\n",
    "#LSTM v1\n",
    "Validation Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.95      0.92     31450\n",
    "           1       0.10      0.05      0.07      3494\n",
    "\n",
    "    accuracy                           0.86     34944\n",
    "   macro avg       0.50      0.50      0.50     34944\n",
    "weighted avg       0.82      0.86      0.84     34944\n",
    "\n",
    "Test Set Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.96      0.93     31450\n",
    "           1       0.11      0.04      0.06      3494\n",
    "\n",
    "    accuracy                           0.87     34944\n",
    "   macro avg       0.51      0.50      0.50     34944\n",
    "weighted avg       0.82      0.87      0.84     34944\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1096/1096 [==============================] - 781s 700ms/step - loss: 0.0474 - val_loss: 1554628736.0000\n",
      "Epoch 2/50\n",
      "1096/1096 [==============================] - 706s 645ms/step - loss: 0.0047 - val_loss: 113374720.0000\n",
      "Epoch 3/50\n",
      "1096/1096 [==============================] - 725s 661ms/step - loss: 0.0038 - val_loss: 65938828.0000\n",
      "Epoch 4/50\n",
      "1096/1096 [==============================] - 691s 630ms/step - loss: 0.0033 - val_loss: 179972653056.0000\n",
      "Epoch 5/50\n",
      "1096/1096 [==============================] - 690s 629ms/step - loss: 0.0030 - val_loss: 2654580736.0000\n",
      "Epoch 6/50\n",
      "1096/1096 [==============================] - 688s 627ms/step - loss: 0.0028 - val_loss: 8348533.0000\n",
      "Epoch 7/50\n",
      "1096/1096 [==============================] - 727s 663ms/step - loss: 0.0026 - val_loss: 15705239.0000\n",
      "Epoch 8/50\n",
      "1096/1096 [==============================] - 703s 642ms/step - loss: 0.0025 - val_loss: 196708768.0000\n",
      "Epoch 9/50\n",
      "1096/1096 [==============================] - 674s 615ms/step - loss: 0.0024 - val_loss: 110391058432.0000\n",
      "Epoch 10/50\n",
      "1096/1096 [==============================] - 651s 594ms/step - loss: 0.0023 - val_loss: 1609223503872.0000\n",
      "Epoch 11/50\n",
      "1096/1096 [==============================] - ETA: 0s - loss: 0.0022"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 111\u001b[0m\n\u001b[0;32m    108\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    118\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Calculate Reconstruction Loss Threshold\u001b[39;00m\n\u001b[0;32m    121\u001b[0m reconstructions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\keras\\engine\\training.py:1606\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1593\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1594\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1604\u001b[0m         steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution,\n\u001b[0;32m   1605\u001b[0m     )\n\u001b[1;32m-> 1606\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1619\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1620\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1621\u001b[0m }\n\u001b[0;32m   1622\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\keras\\engine\\training.py:1947\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1943\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1944\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1945\u001b[0m ):\n\u001b[0;32m   1946\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1947\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1949\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateful_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\shahn\\miniconda3\\envs\\energy_fraud\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#LSTM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load and prepare data\n",
    "file_path = 'LD2011_2014.txt'\n",
    "data = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=';',\n",
    "    parse_dates=[0],\n",
    "    index_col=0,\n",
    "    dayfirst=True,\n",
    "    decimal=','\n",
    ")\n",
    "data_kwh = data / 4  # Convert to kWh\n",
    "\n",
    "# Ensure the index is a datetime format for all datasets\n",
    "data_kwh.index = pd.to_datetime(data_kwh.index)\n",
    "\n",
    "# Split data\n",
    "train_data = data_kwh['2011-01-01':'2012-12-31']\n",
    "val_data = data_kwh['2013-01-01':'2013-12-31']\n",
    "test_data = data_kwh['2014-01-01':'2014-12-31']\n",
    "\n",
    "# Anomaly Injection\n",
    "def inject_and_label_anomalies(data, anomaly_percentage=0.15, spike_factor=1.5, prolonged_anomaly_duration=4):\n",
    "    np.random.seed(42)\n",
    "    data_with_anomalies = data.copy()\n",
    "    data_with_anomalies['anomaly'] = 0\n",
    "    num_anomalies = int(len(data) * anomaly_percentage)\n",
    "\n",
    "    # Inject sudden spikes and dips\n",
    "    for _ in range(num_anomalies):\n",
    "        idx = np.random.choice(data.index)\n",
    "        col = np.random.choice(data.columns)\n",
    "        if np.random.rand() > 0.5:\n",
    "            data_with_anomalies.at[idx, col] *= spike_factor\n",
    "        else:\n",
    "            data_with_anomalies.at[idx, col] /= spike_factor\n",
    "        data_with_anomalies.at[idx, 'anomaly'] = 1\n",
    "\n",
    "    # Inject prolonged anomalies\n",
    "    prolonged_indices = np.random.choice(data.index[:-prolonged_anomaly_duration], num_anomalies // 2, replace=False)\n",
    "    for idx in prolonged_indices:\n",
    "        if not isinstance(idx, pd.Timestamp):  # Ensure index is a Timestamp\n",
    "            continue\n",
    "        col = np.random.choice(data.columns)\n",
    "        for i in range(prolonged_anomaly_duration):\n",
    "            data_with_anomalies.at[idx + pd.Timedelta(minutes=15 * i), col] *= spike_factor\n",
    "            data_with_anomalies.at[idx + pd.Timedelta(minutes=15 * i), 'anomaly'] = 1\n",
    "\n",
    "    return data_with_anomalies\n",
    "\n",
    "# Inject anomalies\n",
    "val_data_with_anomalies = inject_and_label_anomalies(val_data)\n",
    "test_data_with_anomalies = inject_and_label_anomalies(test_data)\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = scaler.fit_transform(np.clip(train_data, -1e10, 1e10))  # Clip extreme values\n",
    "val_data_scaled = scaler.transform(np.clip(val_data_with_anomalies.drop(columns=['anomaly']), -1e10, 1e10))\n",
    "test_data_scaled = scaler.transform(np.clip(test_data_with_anomalies.drop(columns=['anomaly']), -1e10, 1e10))\n",
    "\n",
    "# Prepare data for LSTM input format (3D array)\n",
    "def create_sequences(data, seq_length=48):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train = create_sequences(train_data_scaled)\n",
    "X_val = create_sequences(val_data_scaled)\n",
    "X_test = create_sequences(test_data_scaled)\n",
    "\n",
    "# Reshape for anomaly labels\n",
    "y_val = val_data_with_anomalies['anomaly'][48:].values  # Shift by seq_length\n",
    "y_test = test_data_with_anomalies['anomaly'][48:].values\n",
    "\n",
    "# Check for NaNs in the dataset before feeding to the model\n",
    "assert not np.isnan(X_train).any(), \"NaNs detected in X_train\"\n",
    "assert not np.isnan(X_val).any(), \"NaNs detected in X_val\"\n",
    "assert not np.isnan(X_test).any(), \"NaNs detected in X_test\"\n",
    "\n",
    "# LSTM Autoencoder Model with Gradient Clipping\n",
    "model = Sequential([\n",
    "    LSTM(128, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(64, activation='relu', return_sequences=False),\n",
    "    RepeatVector(X_train.shape[1]),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(128, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(X_train.shape[2]))\n",
    "])\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)  # Reduced learning rate with gradient clipping\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate Reconstruction Loss Threshold\n",
    "reconstructions = model.predict(X_train)\n",
    "train_loss = np.mean(np.square(reconstructions - X_train), axis=(1, 2))\n",
    "threshold = np.percentile(train_loss, 95)\n",
    "\n",
    "# Predict Anomalies on Validation and Test Sets\n",
    "def detect_anomalies(model, data, threshold):\n",
    "    reconstructions = model.predict(data)\n",
    "    loss = np.mean(np.square(reconstructions - data), axis=(1, 2))\n",
    "    return (loss > threshold).astype(int), loss\n",
    "\n",
    "val_preds, val_loss = detect_anomalies(model, X_val, threshold)\n",
    "test_preds, test_loss = detect_anomalies(model, X_test, threshold)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Validation Set Classification Report:\")\n",
    "print(classification_report(y_val, val_preds))\n",
    "\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, test_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0053\n",
      "Epoch 1: val_loss improved from inf to 0.00446, saving model to best_model.h5\n",
      "958/958 [==============================] - 610s 630ms/step - loss: 0.0053 - val_loss: 0.0045\n",
      "Epoch 2/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0042\n",
      "Epoch 2: val_loss improved from 0.00446 to 0.00443, saving model to best_model.h5\n",
      "958/958 [==============================] - 586s 611ms/step - loss: 0.0042 - val_loss: 0.0044\n",
      "Epoch 3/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 3: val_loss improved from 0.00443 to 0.00440, saving model to best_model.h5\n",
      "958/958 [==============================] - 601s 627ms/step - loss: 0.0041 - val_loss: 0.0044\n",
      "Epoch 4/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0041\n",
      "Epoch 4: val_loss improved from 0.00440 to 0.00428, saving model to best_model.h5\n",
      "958/958 [==============================] - 589s 615ms/step - loss: 0.0041 - val_loss: 0.0043\n",
      "Epoch 5/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 5: val_loss did not improve from 0.00428\n",
      "958/958 [==============================] - 592s 618ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 6: val_loss improved from 0.00428 to 0.00424, saving model to best_model.h5\n",
      "958/958 [==============================] - 587s 613ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "Epoch 7/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 7: val_loss did not improve from 0.00424\n",
      "958/958 [==============================] - 587s 613ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 8/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 8: val_loss did not improve from 0.00424\n",
      "958/958 [==============================] - 585s 611ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 9/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 9: val_loss did not improve from 0.00424\n",
      "958/958 [==============================] - 588s 614ms/step - loss: 0.0040 - val_loss: 0.0044\n",
      "Epoch 10/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 10: val_loss did not improve from 0.00424\n",
      "958/958 [==============================] - 585s 611ms/step - loss: 0.0040 - val_loss: 0.0043\n",
      "Epoch 11/50\n",
      "958/958 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 11: val_loss did not improve from 0.00424\n",
      "958/958 [==============================] - 588s 614ms/step - loss: 0.0040 - val_loss: 0.0042\n",
      "3832/3832 [==============================] - 355s 92ms/step\n",
      "822/822 [==============================] - 85s 103ms/step\n",
      "Test Set Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.95      0.79     17561\n",
      "           1       0.40      0.07      0.11      8713\n",
      "\n",
      "    accuracy                           0.66     26274\n",
      "   macro avg       0.54      0.51      0.45     26274\n",
      "weighted avg       0.58      0.66      0.56     26274\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Parameters for synthetic data generation\n",
    "num_meters = 5\n",
    "num_days = 5 * 365  # 5 years of data\n",
    "sampling_rate = 15  # in minutes\n",
    "anomaly_percentage = 0.1  # 10% anomalies\n",
    "\n",
    "# Calculate total samples based on days and sampling rate\n",
    "total_samples = num_days * 24 * (60 // sampling_rate)\n",
    "time_index = pd.date_range(start=\"2010-01-01\", periods=total_samples, freq=f\"{sampling_rate}T\")\n",
    "\n",
    "# Initialize DataFrame with time index\n",
    "data = pd.DataFrame(index=time_index)\n",
    "\n",
    "# Generate baseline consumption patterns\n",
    "for meter_id in range(1, num_meters + 1):\n",
    "    daily_pattern = 0.5 + 0.2 * np.sin(np.linspace(0, 2 * np.pi, 24 * (60 // sampling_rate))) * (1 + np.random.normal(0, 0.01))\n",
    "    weekly_pattern = 0.5 + 0.1 * np.sin(np.linspace(0, 2 * np.pi, 7 * 24 * (60 // sampling_rate))) * (1 + np.random.normal(0, 0.01))\n",
    "    \n",
    "    daily_pattern_repeated = np.tile(daily_pattern, num_days)\n",
    "    weekly_pattern_repeated = np.tile(weekly_pattern, num_days // 7 + 1)[:total_samples]\n",
    "    \n",
    "    baseline_consumption = daily_pattern_repeated + weekly_pattern_repeated[:total_samples] + np.random.normal(0, 0.05, total_samples)\n",
    "    data[f'meter_{meter_id}'] = baseline_consumption\n",
    "\n",
    "# Function to inject anomalies\n",
    "def inject_anomalies(data, anomaly_percentage, spike_factor=3, dip_factor=0.3, prolonged_anomaly_duration=4):\n",
    "    num_anomalies = int(len(data) * anomaly_percentage)\n",
    "    anomaly_indices = np.random.choice(data.index, size=num_anomalies, replace=False)\n",
    "    \n",
    "    data['anomaly'] = 0  # Column to track anomalies\n",
    "    \n",
    "    for idx in anomaly_indices:\n",
    "        col = np.random.choice(data.columns[:-1])  # Random meter column\n",
    "        \n",
    "        if np.random.rand() > 0.5:\n",
    "            data.loc[idx, col] *= spike_factor  # Sudden spike\n",
    "        else:\n",
    "            data.loc[idx, col] *= dip_factor  # Sudden dip\n",
    "            \n",
    "        # Prolonged anomalies\n",
    "        for i in range(prolonged_anomaly_duration):\n",
    "            future_idx = idx + pd.Timedelta(minutes=sampling_rate * i)\n",
    "            if future_idx in data.index:\n",
    "                data.loc[future_idx, col] *= spike_factor if np.random.rand() > 0.5 else dip_factor\n",
    "                data.loc[future_idx, 'anomaly'] = 1\n",
    "\n",
    "    return data\n",
    "\n",
    "# Apply anomaly injection\n",
    "data_with_anomalies = inject_anomalies(data.copy(), anomaly_percentage)\n",
    "\n",
    "# Data Scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data_with_anomalies.drop(columns=['anomaly']))\n",
    "\n",
    "# Convert data into supervised learning format for LSTM\n",
    "def create_sequences(data, seq_length=48):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data_with_anomalies['anomaly'].iloc[i + seq_length])  # Label is the next point\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 48  # 12 hours of 15-minute intervals\n",
    "X, y = create_sequences(scaled_data, seq_length=seq_length)\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# LSTM Autoencoder Model\n",
    "model = Sequential([\n",
    "    LSTM(64, activation='relu', input_shape=(seq_length, X.shape[2]), return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32, activation='relu', return_sequences=False),\n",
    "    RepeatVector(seq_length),\n",
    "    LSTM(32, activation='relu', return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(X.shape[2]))\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=5,  # Number of epochs to wait before stopping if no improvement\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best validation loss\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_model.h5',  # Path to save the best model\n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Add the callbacks to your fit function\n",
    "history = model.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=50,  # Adjust as needed\n",
    "    batch_size=128,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=[early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Set threshold for anomaly detection\n",
    "X_train_pred = model.predict(X_train)\n",
    "train_mae_loss = np.mean(np.abs(X_train_pred - X_train), axis=(1, 2))\n",
    "threshold = np.percentile(train_mae_loss, 95)\n",
    "\n",
    "# Evaluate on test set\n",
    "X_test_pred = model.predict(X_test)\n",
    "test_mae_loss = np.mean(np.abs(X_test_pred - X_test), axis=(1, 2))\n",
    "y_test_pred = (test_mae_loss > threshold).astype(int)\n",
    "\n",
    "# Classification report\n",
    "print(\"Test Set Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
